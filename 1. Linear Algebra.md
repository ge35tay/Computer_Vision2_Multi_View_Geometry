# 1. Linear Algebra

## 1. Vector Space

A set $V$ is called a ==Linear Space== or ==a vector space over the field over $\mathbb R$== if a) it is closed (end up still in space) under vector summation(**commutative group**) and
$$
+: V \times V \rightarrow V
$$
b) scalar multiplication:
$$
\cdot: \mathbb R \times V \rightarrow V
$$
In summary we can prove the vector space using:
$$
\alpha v_1 + \beta v_2 \in V \quad \forall v_1,v_2 \in V, \quad \forall \alpha, \beta \in \mathbb R
$$
A subset $W$ $\in V$ of a vector space $V$ is called a subspace of  $V$ if  $0 \in W$ and $W$ is closed under + and $\cdot$

### **1.1 Linear dependency and Basis**

1. The spanned subspace of a set of vectors $S = \left\{ v_1, ... v_k \right\} \in V$ is the subspace formed by **all linear combinations of these vectors**
   $$
   span(S)= \left\{ v \in V | v = \sum_{i=1}^k \alpha_i v_i \right\}
   $$
   the set $S$ is called **linear independent** if:
   $$
   \sum_{i=1}^k \alpha_i v_i = 0 \Rightarrow \alpha_i=0, \forall i
   $$
   we can prove the linear independency with this

2. A set of vector $B = \left\{ v_1, ... v_n \right\}$  is called **basis of $V$** if a) it is linearly independent and b) if it spans the whole vector space $V$ . A basis is the maximal set of linearly independent vectors

### 1.2 properties of Basis

let $B $ and $B'$ be two bases of a linear space $V$

   - $B $ and $B'$  contains **the same number n of vectors**, this number is called the dimension of the linear space  $V$

   -  Any vector $v \in V$ can be uniquely expressed as a linear combination of the basis vectors in  $B = \left\{ b_1, ... b_n \right\}$
     $$
     v = \sum_{i=1}^k \alpha_i b_i
     $$
     
     unique expression:
     $$
     \sum_{i=1}^k \alpha_k v_k = \sum_{i=1}^k \beta_k v_k \\
     \sum_{i=1}^k(\alpha_k- \beta_k)v_k = 0 \Rightarrow \alpha_k =\beta_k
     $$
     
   - All vectors of $B$ can be expressed as linear combinations of vectors of another basis $b'_i \in B'$
     $$
     b'_i =  \sum_{j=1}^n \alpha_{ji} b_i
     $$
     $\alpha_{ji}$ can be combined as the **basis transform matrix A**: B' = BA

### 1.3 Inner product / dot product


$$
<\cdot,\cdot>: V \times V \rightarrow \mathbb R
$$


which is defined by 3 properties:

1. **linear**: $<u,\alpha v + \beta w> = \alpha <u,v> + \beta <u,w>$
2. **symmetric**: $<u,v> = <v,u>$
3. **positive definite**: $<v,v> \ge 0$ and  $<v,v> = 0 \Leftrightarrow v=0. $**norm of a vector** $|v| = \sqrt(<v,v>)$

metric to measure length and distances:
$$
d = V \times V \rightarrow \mathbb R, d(v,w) = |v-w|
$$

### 1.4 Canonical Induced inner product:

On $V = \mathbb R^n$, one can define the canonical basis $B = I_n$ as:
$$
<x,y> = x^T y =  \sum_{i=1}^n x_iy_i
$$
which induces the standard L2 norm or Euclidean norm:
$$
|x|_2 = \sqrt(x^Tx) = \sqrt(x_1^2 + \cdot \cdot \cdot x_n^2)
$$


Two vectors v and w are ==orthogonal== iff $<v,w> = 0$   



### 1.5 Kronecker Product and stack of a Matrix

Given two matrices $A \in \mathbb R^{m \times n}$ and $B \in \mathbb R^{k \times l}$, one can define their ==kronecker Product== $A \otimes B$ by:
$$
A \otimes B = \left(
 \begin{matrix}
   a_{11}B & \cdot \cdot \cdot & & a_{1n}B\\
   \cdot & \cdot &    &  \cdot\\
   \cdot &  & \cdot &  \cdot \\
   a_{m1}B & \cdot & \cdot &  a_{mn}B
  \end{matrix}
\right) \in \mathbb R ^{mk \times nl}
$$
in matlab command:

```matlab
C = kron(A, B)
```

Given a matrix $A \in \mathbb R^{m \times n}$ , its ==stack== $A^s$ is obtained by stacking its n column vectors $a_1, ..., a_n \in R^m$ 

### 1.6 Linear transformations and Matrices

Linear transformation between 2 linear spaces, usually expressed by matrices

A ==linear transformation== L between 2 linear space $V$ and $W$ is a map $L: V \rightarrow W$ such that:
$$
L(x+y) = L(x) + L(y), \quad \forall x,y \in V \\
L(\alpha x) = \alpha L(x), \quad \forall x \in V, \alpha \in \mathbb R
$$
So, **the action of linear transform is uniquely defined by its action on the basis vector of v**, that's to say we can regard the transformation matrix as $(L(b_1),...L(b_n))$

### 1.7 The Linear Group $GL(n)$ and $SL(n)$

A ==Group== is a set $G$ with an operation $o$: $G \times G \rightarrow G$ such that

- **closed** : $g_1 \, o \, g_2 \in G \quad \forall g_1,g_2 \in G$
- **associative**:  $g_1 \, o \,(g_2 \, o \, g_3 )= (g_1 \, o \, g_2)\, o \,g_3 \quad \forall  g_1,g_2, g_3 \in G$
- **neutral**: $\exist e \in G: e \, o \,g = g\, o \,e $
- **inverse**: $\exist g^{-1} \in G: g \, o \, g^{-1} = g^{-1} \, o \, g = e \quad \forall g \in G $

eg. Intergral with operation addition forms a (discrete) group; square matrix with matrix form a gorup (multiplication iff A is invertiable)

neural number does not form a group because it has not minus (not inverse)

Example: 

(1) ==general linear group== $GL(n)$ : all invertible real $n \times n$ matrix form a group w.r.t matrix multiplication.

(2) ==special linear group== $SL(n)$ : All matrices $A \in GL(n)$ which $det(A) = 1$ . because $det(A^{-1}) = det(A)^{-1}$, so the inverse of A is also in special linear group (subgroup of general linear group)

(3) ==Affine Group== $A(n)$: represent the abstract group in affine transformation: A matrix $A \in GL(n)$ and a vector $b \in \mathbb R^n$ such that:
$$
L(x) = Ax + b
$$
The set of such affine transformations is called the ==affine group of dimension n, A(n)==

We can introduce the **homogeneous coordinates** to represent $x \in \mathbb R$ by $(x; 1) \in R^{n+1}$

such a matrix in homo coord is called the affine matrix, is an element of $GL(n+1)$

(4) ==Orthogonal Group== $O(n)$: a matrix A is called orthogonal if it preserves the inner product $<Ax,Ay> = <x,y>$ ==> det(R) = plus or minus 1.

For an orthogonal matrix we have:
$$
<Rx,Ry> = x^TR^TRy = x^Ty
$$

- The subgroup of $O(n)$ with $det(R) = +1$ is called the ==special orthogonal group==

(5) ==Euclidean Group E(n)==. A Euclidean transformation $L(n)$ from $\mathbb R^n$ to  $\mathbb R^n$ is defined by an orthogonal matrix $R \in O(n)$ and a vector $T \in \mathbb R^n$

The set of such transformation is called **Euclidean Group E(n)**.

If $R \in SO(n)$, then we have **special euclidean group** SE(n). det(R) = -1 is the reflexion, is not a rotation (change the direction of object !!!)

if particular $SE(3)$ represents the rigid body motion.
$$
SO(n) \subset O(n) \subset GL(n), \quad SE(n) \subset E(n) \subset A(n) \subset GL(n+1)
$$


-  **Matrix Representation of Groups**: A group $G$ has a matrix representation or can be realized as a matrix group if there exists an injective transformation: 
  $$
  R : G \rightarrow GL(n)
  $$
  which preserve the structure of group structure of G, that is inverse and composition are preserved by the map:
  $$
  R(e) = I_n \qquad R(g\,o\,h) = R(g)R(h)
  $$
  such a $R$ is called ==group homomorphiosm==

  ALLOW US TO ANALYZE THE ABSTRACT GROUP USING LINEAR ALGEBRA. (rotation of camera in rotation matrix)

### 1.8 Range, span, null space and kernel

Let $A \in R^{m \times n}$ be a matrix defining a linear map from $\mathbb R^ n \rightarrow \mathbb R^ m$

The range or span of  A (1.2 span of a groups of vector) is defined as the subspace of $R^m$ that can be reached by A: 
$$
range(A) = \left\{ y \in R^m | \exists x \in \mathbb R^n: Ax=y \right\}
$$
Also is the span of its colume vectors

The null space or kernel of a matrix A is given by the subset of vectors $x \in R^n$ which is mapped to 0 in dimention m:
$$
null(A) = ker(A) = \left\{ x \in \mathbb R^n | Ax = 0\right\}
$$




- the concept of range and null is useful for solution of linear equations $Ax = b$:
  - will have solution $x \in R^n$ iff $b \in range(A)$
  - the solution will be unique iff $ker(A) = {0}$. Otherwise it can have multiple solutions

### 1.9 Rank of a matrix

The rank of a matrix is the dimension of its range

- $rank(A) = n- dim(ker(A))$

- $0\le rank(A) \le min(m,n)$

### 1.10 Eigenvalues and Eigenvectors

$Av = \lambda v$ , only for quadratic matrix A

The spectrum $\sigma(A)$ of a matrix A is the set of all its eigenvalues.

Let A be a square matrix then：

- 1. if $Av = \lambda v$  for some $\lambda \in R$, then there exists also left eigenvector $\eta \in \mathbb R^T$: $\eta ^T A = \lambda \eta ^T$, hence $\sigma(A) = \sigma(A^T)$

- 2. The eigenvectors of a matrix A associated with different eigenvalues are linearly independent -> the set of eigenvectors of A form a basis of space
- 3. det(A) is equal to product of all eigenvalues
- 4. if $B = PAP^{-1}$ for some nonsingular matrix P, then $\sigma(A) = \sigma(B)$

- 5. if $\lambda$ is an eigenvalue, then it,s conjugate is also a eigenvalue. ==> even the eigenvector is real, it does not mean that the eigenvalue is real

- Eigenvalue and Eigenvector for **symmetric matrix S**

​		A matrix $S \in \mathbb R^{n \times n}$ is called **Symmetric** if $S^T = S$ . symetric matrix can be positive semi-definitive or positive definitive

​	Let S is a real symmetric matrix then:

> 1. All eigenvalue of S are real
> 2. Eigenvectors $v_i$  and $v_j$ of S corresponding to distinct eigenvalues are orthogonal
> 3. There always exist n orthonormal eigenvectors of $S$ which form a basis of $R^n$. Even the eigenvalues are the same, we can choose different eigenvectors for the individual eigenvalue. Let the $V = (v_1,...v_n) \in O(n)$ be the orthogonal matrix of these eigenvectors, and $\land = diag \left\{\lambda_1, ..., \lambda_n\right\}$  is the diagonal matrix of eigenvalues, then: >$S = V \land V$  ==only for symmetric and square matrix==
> 4. S is positive definte (semił) if all the eigenvalues are positive (nonnegative)
> 5. Let S be positive semi-definite and $\lambda_1$, $\lambda_n$ the largest and smallest eigenvalue. Then $\lambda_1 = max_{|x| =1}<x, Sx>$ , and $\lambda_n = min_{|x| =1}<x, Sx>$ 

### 1.11 Norm of Matrix

There are many ways to define the norm of $A  \in \mathbb R^{m \times n}$ 

In particular the **induced 2-norm of a matrix A** is defined as:
$$
||A||_2 = max_{|x|_2 = 1}|Ax|_2= max_{|x|_2 = 1} \sqrt{<x, A^TAx>}
$$
   the **Frobenius Norm of A ** as:
$$
||A||_f = \sqrt{(trace(A^T A))}
$$
since $A^TA$ is symmetric and non-negative, $A^TA = V diag\left\{\sigma_1^2, ...,  \sigma_n^2\right\}V^T$

so the induced 2-norm of a matrix is $\sigma_1$,

Frobenius norm is $\sqrt{\sigma_1^2 +... + \sigma_n^2}$

### 1.11 Skew-symmetric matrices

A matrix A is called skew-symmetric anti-symmetric if $A^T = -A$, ==>  ==the elements in the diagonal of A are all 0==.

If A is a real skew-symmetric matrix then:

- 1. All eigenvalues of A are either 0 or purely imaginary

- 2. There exists an orthogonal matrix V such that $A = V \land V^T$, where $\land$ is a block-diagonal matrix.

     $\land= diag \left\{ A_1,...,A_m, 0, 0, 0 \right\}$ with real skew-symmetric matrices $A_i$  of the form:
     $$
     A_i = (
      \begin{matrix}
        0 & a_i\\ 
        -a_i & 0 
       \end{matrix} 
     ) \in R^{2 \times 2}, i = 1,...,m
     $$

### 1.12 The Singular Value Decomposition (SVD)

SVD can be seen as a generalization of eigenvalues and eigenvectors of non-square matrices.

#### 1.12.1 Algebraic Derivation of SVD

Let $A \in \mathbb R^{m \times n}$ with $m \ge n$ be a matrix of rank(A) = p, then there exists:

- $U \in \mathbb R^{m \times p}$ whose columns are orthonormal
- $V \in \mathbb R^{n \times p}$ whose columns are orthonormal (norm = 1)
- $\Sigma \in \mathbb R^{p \times p}, \Sigma = diag \left\{ \sigma_1,...\sigma_p\right\}$  with $\sigma_1 \ge..., \ge\sigma_p$

such that $A = U \Sigma V^T$

proof and geometric interpretation see Page 24, 25

- Generalized Inverse

  for $A = U \Sigma V^T$, its pseudo inverse can be defined as:
  $$
  A^{+} = V \Sigma^{+}U^T,  \quad \Sigma^{+}= (
   \begin{matrix}
     \Sigma_1^{-1} & 0\\ 
     0 & 0 
    \end{matrix} 
  )
  $$
